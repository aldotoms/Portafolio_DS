{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan de Trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importación de librerías y carga de datos\n",
    "El proyecto inicia con la importación de las librerías fundamentales para el análisis y manipulación de datos. Pandas y NumPy permitirán trabajar con estructuras tabulares y cálculos numéricos, mientras que Matplotlib y Seaborn facilitarán la creación de visualizaciones que apoyen la comprensión inicial de los datos. En esta etapa también se cargan los archivos en bruto, preparando el entorno para el análisis posterior.\n",
    "\n",
    "### 2. Exploración de los datos (EDA)\n",
    "La exploración de datos busca comprender la estructura, calidad y comportamiento de la información disponible. Se utilizará Sweetviz para obtener una visión general rápida, complementada con análisis manual mediante Pandas y visualizaciones personalizadas. Se revisarán tipos de datos, estadísticas descriptivas, consistencia entre columnas, presencia de nulos, duplicados y valores atípicos, siempre considerando el contexto del negocio. Este análisis permitirá identificar patrones relevantes y preparar adecuadamente los datos para su transformación posterior, manteniendo copias de seguridad para preservar la integridad de la información original.\n",
    "\n",
    "### 3. Preprocesamiento de los datos\n",
    "En esta fase se reorganizan y combinan los datasets utilizando sus llaves primarias, asegurando una estructura coherente para el modelado. Se generarán nuevas características relevantes, como la antigüedad del cliente, duración del contrato y montos asociados a los servicios contratados. Posteriormente, toda la información se integrará en un único dataframe que concentre las variables necesarias para el análisis predictivo. Este proceso sienta las bases para que los modelos puedan aprender patrones significativos relacionados con la cancelación de clientes.\n",
    "\n",
    "### 4. Modelado de datos\n",
    "El modelado comienza con la definición del conjunto de características y la variable objetivo, seguido de la división del dataset en entrenamiento y prueba. Sobre el conjunto de entrenamiento se aplicarán técnicas de balanceo, escalado y selección de características mediante pipelines para evitar fuga de datos. Se entrenará un conjunto diverso de modelos, desde regresión logística hasta algoritmos de boosting, incluyendo un modelo Dummy como referencia mínima. CatBoost se evaluará tanto con como sin codificación categórica. PyCaret servirá como apoyo para comparar rápidamente el rendimiento de los modelos, mientras que la evaluación formal se basará en métricas como AUC‑ROC, Recall, Precision, F1 y la matriz de confusión.\n",
    "\n",
    "### 5. Comparación de resultados y conclusiones finales\n",
    "Una vez entrenados todos los modelos, se realizará una comparación exhaustiva de su desempeño para identificar el que ofrezca el mejor equilibrio entre capacidad predictiva, estabilidad y utilidad práctica. Se documentarán las razones técnicas que justifican su selección, incluyendo la importancia de las variables y su comportamiento frente al desbalance de clases. Finalmente, se presentarán conclusiones generales del proyecto y recomendaciones para el despliegue en producción, abordando aspectos como monitoreo, actualización del modelo y oportunidades de mejora futura. Con ello, el proyecto se cerrará con una propuesta sólida y alineada a los objetivos estratégicos de Interconnect.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
